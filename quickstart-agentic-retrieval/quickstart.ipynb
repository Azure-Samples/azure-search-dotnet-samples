{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8332ffe",
   "metadata": {},
   "source": [
    "# Quickstart: Agentic retrieval in Azure AI Search\n",
    "\n",
    "Use this notebook to get started with [agentic retrieval](https://learn.microsoft.com/azure/search/search-agentic-retrieval-concept) in Azure AI Search, which integrates conversation history and large language models (LLMs) on Azure OpenAI to plan, retrieve, and synthesize complex queries.\n",
    "\n",
    "Steps in this notebook include:\n",
    "\n",
    "+ Creating an `earth_at_night` search index.\n",
    "\n",
    "+ Loading the index with documents from a GitHub URL.\n",
    "\n",
    "+ Creating an `earth-search-agent` in Azure AI Search that points to an LLM for query planning.\n",
    "\n",
    "+ Using the agent to fetch and rank relevant information from the index.\n",
    "\n",
    "+ Generating answers using the Azure OpenAI client.\n",
    "\n",
    "This notebook provides a high-level demonstration of agentic retrieval. For more detailed guidance, see [Quickstart: Run agentic retrieval in Azure AI Search](https://learn.microsoft.com/azure/search/search-get-started-agentic-retrieval)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f1a5e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "+ An [Azure AI Search service](https://learn.microsoft.com/azure/search/search-create-service-portal) on the Basic tier or higher with [semantic ranker enabled](https://learn.microsoft.com/azure/search/semantic-how-to-enable-disable).\n",
    "\n",
    "+ An [Azure OpenAI resource](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource).\n",
    "\n",
    "+ A [supported model](https://learn.microsoft.com/azure/search/search-agentic-retrieval-how-to-create#supported-models) deployed to your Azure OpenAI resource. This notebook uses `gpt-4o-mini`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db21c7",
   "metadata": {},
   "source": [
    "## Configure access\n",
    "\n",
    "This notebook assumes authentication and authorization using Microsoft Entra ID and role assignments. It also assumes that you run the code from your local device.\n",
    "\n",
    "To configure role-based access:\n",
    "\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com).\n",
    "\n",
    "1. [Enable role-based access](https://learn.microsoft.com/azure/search/search-security-enable-roles) on your Azure AI Search service.\n",
    "\n",
    "1. [Create a system-assigned managed identity](https://learn.microsoft.com/azure/search/search-howto-managed-identities-data-sources#create-a-system-managed-identity) on your Azure AI Search service.\n",
    "\n",
    "1. On your Azure AI Search service, [assign the following roles](https://learn.microsoft.com/azure/search/search-security-rbac#how-to-assign-roles-in-the-azure-portal) to yourself.\n",
    "\n",
    "   + **Owner/Contributor** or **Search Service Contributor**\n",
    "\n",
    "   + **Search Index Data Contributor**\n",
    "\n",
    "   + **Search Index Data Reader**\n",
    "\n",
    "1. On your Azure OpenAI resource, assign **Cognitive Services User** to the managed identity of your search service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ece34",
   "metadata": {},
   "source": [
    "## Set up connections\n",
    "\n",
    "The `sample.env` file contains environment variables for connections to Azure AI Search and Azure OpenAI. Agentic retrieval requires these connections for document retrieval, query planning, query execution, and answer generation.\n",
    "\n",
    "To set up connections:\n",
    "\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com).\n",
    "\n",
    "2. Retrieve the endpoints for both Azure AI Search and Azure OpenAI.\n",
    "\n",
    "3. Save the `sample.env` file as `.env` on your local device.\n",
    "\n",
    "4. Update the `.env` file with the retrieved endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3ddfe",
   "metadata": {},
   "source": [
    "## Install packages and load connections\n",
    "\n",
    "This step installs the packages for this notebook and establishes connections to Azure AI Search and Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8a3b0",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Azure.Search.Documents, 11.7.0-beta.4\"\n",
    "#r \"nuget: Azure.Identity, 1.14.0\"\n",
    "#r \"nuget: Azure.AI.OpenAI, 2.1.0\"\n",
    "#r \"nuget:dotenv.net, 3.2.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4a83d4",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using dotenv.net;\n",
    "using Azure.Identity;\n",
    "\n",
    "// .env should be in the same directory as this notebook\n",
    "DotEnv.Load(options: new DotEnvOptions(envFilePaths: new[] { \".env\" }, ignoreExceptions: false));\n",
    "\n",
    "// Get environment variables with defaults where appropriate\n",
    "string answerModel = Environment.GetEnvironmentVariable(\"ANSWER_MODEL\") ?? \"gpt-4o\";\n",
    "string endpoint = Environment.GetEnvironmentVariable(\"AZURE_SEARCH_ENDPOINT\") \n",
    "    ?? throw new InvalidOperationException(\"AZURE_SEARCH_ENDPOINT is not set.\");\n",
    "var credential = new DefaultAzureCredential();\n",
    "\n",
    "string indexName = Environment.GetEnvironmentVariable(\"AZURE_SEARCH_INDEX\") ?? \"earth_at_night\";\n",
    "string azureOpenAiEndpoint = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\") \n",
    "    ?? throw new InvalidOperationException(\"AZURE_OPENAI_ENDPOINT is not set.\");\n",
    "string azureOpenAiGptDeployment = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_GPT_DEPLOYMENT\") ?? \"gpt-4o\";\n",
    "string azureOpenAiGptModel = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_GPT_MODEL\") ?? \"gpt-4o\";\n",
    "string azureOpenAiApiVersion = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\") ?? \"2025-03-01-preview\";\n",
    "string azureOpenAiEmbeddingDeployment = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\") ?? \"text-embedding-3-large\";\n",
    "string azureOpenAiEmbeddingModel = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_EMBEDDING_MODEL\") ?? \"text-embedding-3-large\";\n",
    "string agentName = Environment.GetEnvironmentVariable(\"AZURE_SEARCH_AGENT_NAME\") ?? \"earth-search-agent\";\n",
    "string apiVersion = \"2025-05-01-Preview\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d85e5a",
   "metadata": {},
   "source": [
    "## Create an index in Azure AI Search\n",
    "\n",
    "This step creates a search index that contains plain text and vector content. You can use an existing index, but it must meet the criteria for [agentic retrieval workloads](https://learn.microsoft.com/azure/search/search-agentic-retrieval-how-to-index). The primary schema requirement is a semantic configuration with a `default_configuration_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23942582",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'earth_at_night' created or updated successfully\r\n"
     ]
    }
   ],
   "source": [
    "using Azure.Search.Documents.Indexes;\n",
    "using Azure.Search.Documents.Indexes.Models;\n",
    "\n",
    "// Define the fields for the index\n",
    "var fields = new List<SearchField>\n",
    "{\n",
    "    new SimpleField(\"id\", SearchFieldDataType.String) { IsKey = true, IsFilterable = true, IsSortable = true, IsFacetable = true },\n",
    "    new SearchField(\"page_chunk\", SearchFieldDataType.String) { IsFilterable = false, IsSortable = false, IsFacetable = false },\n",
    "    new SearchField(\"page_embedding_text_3_large\", SearchFieldDataType.Collection(SearchFieldDataType.Single)) { VectorSearchDimensions = 3072, VectorSearchProfileName = \"hnsw_text_3_large\" },\n",
    "    new SimpleField(\"page_number\", SearchFieldDataType.Int32) { IsFilterable = true, IsSortable = true, IsFacetable = true }\n",
    "};\n",
    "\n",
    "// Define the vectorizer\n",
    "var vectorizer = new AzureOpenAIVectorizer(vectorizerName: \"azure_openai_text_3_large\")\n",
    "{\n",
    "    Parameters = new AzureOpenAIVectorizerParameters\n",
    "    {\n",
    "        ResourceUri = new Uri(azureOpenAiEndpoint),\n",
    "        DeploymentName = azureOpenAiEmbeddingDeployment,\n",
    "        ModelName = azureOpenAiEmbeddingModel\n",
    "    }\n",
    "};\n",
    "\n",
    "// Define the vector search profile and algorithm\n",
    "var vectorSearch = new VectorSearch()\n",
    "{\n",
    "    Profiles =\n",
    "    {\n",
    "        new VectorSearchProfile(\n",
    "            name: \"hnsw_text_3_large\",\n",
    "            algorithmConfigurationName: \"alg\"\n",
    "        )\n",
    "        {\n",
    "            VectorizerName = \"azure_openai_text_3_large\"\n",
    "        }\n",
    "    },\n",
    "    Algorithms =\n",
    "    {\n",
    "        new HnswAlgorithmConfiguration(name: \"alg\")\n",
    "    },\n",
    "    Vectorizers =\n",
    "    {\n",
    "        vectorizer\n",
    "    }\n",
    "};\n",
    "\n",
    "// Define semantic configuration\n",
    "var semanticConfig = new SemanticConfiguration(\n",
    "    name: \"semantic_config\",\n",
    "    prioritizedFields: new SemanticPrioritizedFields\n",
    "    {\n",
    "        ContentFields = { new SemanticField(\"page_chunk\") }\n",
    "    }\n",
    ");\n",
    "\n",
    "var semanticSearch = new SemanticSearch()\n",
    "{\n",
    "    DefaultConfigurationName = \"semantic_config\",\n",
    "    Configurations =\n",
    "    {\n",
    "        semanticConfig\n",
    "    }\n",
    "};\n",
    "\n",
    "// Create the index\n",
    "var index = new SearchIndex(indexName)\n",
    "{\n",
    "    Fields = fields,\n",
    "    VectorSearch = vectorSearch,\n",
    "    SemanticSearch = semanticSearch\n",
    "};\n",
    "\n",
    "// Create the index client and create or update the index\n",
    "var indexClient = new SearchIndexClient(new Uri(endpoint), credential);\n",
    "await indexClient.CreateOrUpdateIndexAsync(index);\n",
    "\n",
    "Console.WriteLine($\"Index '{indexName}' created or updated successfully\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3c782",
   "metadata": {},
   "source": [
    "## Upload sample documents\n",
    "\n",
    "This notebook uses data from NASA's Earth at Night e-book. The data is retrieved from the [azure-search-sample-data](https://github.com/Azure-Samples/azure-search-sample-data) repository on GitHub and passed to the search client for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f40ac7",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents uploaded to index 'earth_at_night'\r\n"
     ]
    }
   ],
   "source": [
    "using System.Net.Http;\n",
    "using System.Text.Json;\n",
    "using Azure.Search.Documents;\n",
    "using Azure.Search.Documents.Indexes;\n",
    "using Azure.Search.Documents.Models;\n",
    "\n",
    "// Download the documents from the GitHub URL\n",
    "string url = \"https://raw.githubusercontent.com/Azure-Samples/azure-search-sample-data/refs/heads/main/nasa-e-book/earth-at-night-json/documents.json\";\n",
    "var httpClient = new HttpClient();\n",
    "var response = await httpClient.GetAsync(url);\n",
    "response.EnsureSuccessStatusCode();\n",
    "var json = await response.Content.ReadAsStringAsync();\n",
    "\n",
    "var documents = JsonSerializer.Deserialize<List<Dictionary<string, object>>>(json);\n",
    "var searchClient = new SearchClient(new Uri(endpoint), indexName, credential);\n",
    "var searchIndexingBufferedSender = new SearchIndexingBufferedSender<Dictionary<string, object>>(\n",
    "    searchClient,\n",
    "    new SearchIndexingBufferedSenderOptions<Dictionary<string, object>>\n",
    "    {\n",
    "        KeyFieldAccessor = doc => doc[\"id\"].ToString(),\n",
    "    }\n",
    ");\n",
    "\n",
    "await searchIndexingBufferedSender.UploadDocumentsAsync(documents);\n",
    "await searchIndexingBufferedSender.FlushAsync();\n",
    "\n",
    "Console.WriteLine($\"Documents uploaded to index '{indexName}'\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af0233",
   "metadata": {},
   "source": [
    "## Create an agent in Azure AI Search\n",
    "\n",
    "This step creates a knowledge agent, which acts as a wrapper for the LLM you deployed to Azure OpenAI. The LLM is used to send queries to an agentic retrieval pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd8d8ed",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "(1,7): error CS0246: The type or namespace name 'Azure' could not be found (are you missing a using directive or an assembly reference?)\r\n(3,28): error CS0246: The type or namespace name 'AzureOpenAIVectorizerParameters' could not be found (are you missing a using directive or an assembly reference?)\r\n(5,27): error CS0103: The name 'azureOpenAiEndpoint' does not exist in the current context\r\n(6,22): error CS0103: The name 'azureOpenAiGptDeployment' does not exist in the current context\r\n(7,17): error CS0103: The name 'azureOpenAiGptModel' does not exist in the current context\r\n(10,22): error CS0246: The type or namespace name 'KnowledgeAgentAzureOpenAIModel' could not be found (are you missing a using directive or an assembly reference?)\r\n(12,23): error CS0246: The type or namespace name 'KnowledgeAgentTargetIndex' could not be found (are you missing a using directive or an assembly reference?)\r\n(12,49): error CS0103: The name 'indexName' does not exist in the current context\r\n(18,17): error CS0246: The type or namespace name 'KnowledgeAgent' could not be found (are you missing a using directive or an assembly reference?)\r\n(19,11): error CS0103: The name 'agentName' does not exist in the current context\r\n(22,7): error CS0103: The name 'indexClient' does not exist in the current context\r\n(23,39): error CS0103: The name 'agentName' does not exist in the current context",
     "output_type": "error",
     "traceback": [
      "(1,7): error CS0246: The type or namespace name 'Azure' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(3,28): error CS0246: The type or namespace name 'AzureOpenAIVectorizerParameters' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(5,27): error CS0103: The name 'azureOpenAiEndpoint' does not exist in the current context\r\n",
      "(6,22): error CS0103: The name 'azureOpenAiGptDeployment' does not exist in the current context\r\n",
      "(7,17): error CS0103: The name 'azureOpenAiGptModel' does not exist in the current context\r\n",
      "(10,22): error CS0246: The type or namespace name 'KnowledgeAgentAzureOpenAIModel' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(12,23): error CS0246: The type or namespace name 'KnowledgeAgentTargetIndex' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(12,49): error CS0103: The name 'indexName' does not exist in the current context\r\n",
      "(18,17): error CS0246: The type or namespace name 'KnowledgeAgent' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(19,11): error CS0103: The name 'agentName' does not exist in the current context\r\n",
      "(22,7): error CS0103: The name 'indexClient' does not exist in the current context\r\n",
      "(23,39): error CS0103: The name 'agentName' does not exist in the current context"
     ]
    }
   ],
   "source": [
    "using Azure.Search.Documents.Indexes.Models;\n",
    "\n",
    "var openAiParameters = new AzureOpenAIVectorizerParameters\n",
    "{\n",
    "    ResourceUri = new Uri(azureOpenAiEndpoint),\n",
    "    DeploymentName = azureOpenAiGptDeployment,\n",
    "    ModelName = azureOpenAiGptModel\n",
    "};\n",
    "\n",
    "var agentModel = new KnowledgeAgentAzureOpenAIModel(azureOpenAIParameters: openAiParameters);\n",
    "\n",
    "var targetIndex = new KnowledgeAgentTargetIndex(indexName)\n",
    "{\n",
    "    DefaultRerankerThreshold = 2.5f\n",
    "};\n",
    "\n",
    "// Create the knowledge agent\n",
    "var agent = new KnowledgeAgent(\n",
    "    name: agentName,\n",
    "    models: new[] { agentModel },\n",
    "    targetIndexes: new[] { targetIndex });\n",
    "await indexClient.CreateOrUpdateKnowledgeAgentAsync(agent);\n",
    "Console.WriteLine($\"Knowledge agent '{agentName}' created or updated successfully\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157c598",
   "metadata": {},
   "source": [
    "## Set up messages\n",
    "\n",
    "Messages are the input for the retrieval route and contain the conversation history. Each message includes a `role` that indicates its origin, such as `assistant` or `user`, and `content` in natural language. The LLM you use determines which roles are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6ff9dc",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string instructions = @\"\n",
    "A Q&A agent that can answer questions about the Earth at night.\n",
    "Sources have a JSON format with a ref_id that must be cited in the answer.\n",
    "If you do not have the answer, respond with \"\"I don't know\"\".\n",
    "\";\n",
    "\n",
    "var messages = new List<Dictionary<string, string>>\n",
    "{\n",
    "    new Dictionary<string, string>\n",
    "    {\n",
    "        { \"role\", \"system\" },\n",
    "        { \"content\", instructions }\n",
    "    }\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21be04",
   "metadata": {},
   "source": [
    "## Use agentic retrieval to fetch results\n",
    "\n",
    "This step runs the retrieval pipeline to extract relevant information from your search index. Based on the messages and parameters on the retrieval request, the LLM:\n",
    "\n",
    "1. Analyzes the entire conversation history to determine the underlying information need.\n",
    "\n",
    "1. Breaks down the compound user query into focused subqueries.\n",
    " \n",
    "1. Runs each subquery simultaneously against text fields and vector embeddings in your index.\n",
    "\n",
    "1. Uses semantic ranker to rerank the results of all subqueries.\n",
    "\n",
    "1. Merges the results into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239a9e12",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using Azure.Search.Documents.Agents;\n",
    "using Azure.Search.Documents.Agents.Models;\n",
    "\n",
    "var agentClient = new KnowledgeAgentRetrievalClient(\n",
    "    endpoint: new Uri(endpoint),\n",
    "    agentName: agentName,\n",
    "    tokenCredential: new DefaultAzureCredential()\n",
    ");\n",
    "\n",
    "messages.Add(new Dictionary<string, string>\n",
    "{\n",
    "    { \"role\", \"user\" },\n",
    "    { \"content\", @\"\n",
    "Why do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown?\n",
    "Why is the Phoenix nighttime street grid is so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?\n",
    "\" }\n",
    "});\n",
    "\n",
    "var retrievalResult = await agentClient.RetrieveAsync(\n",
    "    retrievalRequest: new KnowledgeAgentRetrievalRequest(\n",
    "            messages: messages\n",
    "                .Where(message => message[\"role\"] != \"system\")\n",
    "                .Select(\n",
    "                message => new KnowledgeAgentMessage(\n",
    "                    role: message[\"role\"],\n",
    "                    content: new[] { new KnowledgeAgentMessageTextContent(message[\"content\"]) }))\n",
    "                .ToList()\n",
    "            )\n",
    "        {\n",
    "            TargetIndexParams = { new KnowledgeAgentIndexParams { IndexName = indexName, RerankerThreshold = 2.5f } }\n",
    "        }\n",
    "    );\n",
    "\n",
    "messages.Add(new Dictionary<string, string>\n",
    "{\n",
    "    { \"role\", \"assistant\" },\n",
    "    { \"content\", (retrievalResult.Value.Response[0].Content[0] as KnowledgeAgentMessageTextContent).Text }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ddaf4f",
   "metadata": {},
   "source": [
    "### Review the retrieval response, activity, and results\n",
    "\n",
    "Each retrieval response from Azure AI Search includes:\n",
    "\n",
    "+ A unified string that represents grounding data from the search results.\n",
    "\n",
    "+ The query plan.\n",
    "\n",
    "+ Reference data that shows which chunks of the source documents contributed to the unified string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80676736",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\"ref_id\":0,\"content\":\"<!-- PageHeader=\\\"Urban Structure\\\" -->\\n\\n### Location of Phoenix, Arizona\\n\\nThe image depicts a globe highlighting the location of Phoenix, Arizona, in the southwestern United States, marked with a blue pinpoint on the map of North America. Phoenix is situated in the central part of Arizona, which is in the southwestern region of the United States.\\n\\n---\\n\\n### Grid of City Blocks-Phoenix, Arizona\\n\\nLike many large urban areas of the central and western United States, the Phoenix metropolitan area is laid out along a regular grid of city blocks and streets. While visible during the day, this grid is most evident at night, when the pattern of street lighting is clearly visible from the low-Earth-orbit vantage point of the ISS.\\n\\nThis astronaut photograph, taken on March 16, 2013, includes parts of several cities in the metropolitan area, including Phoenix (image right), Glendale (center), and Peoria (left). While the major street grid is oriented north-south, the northwest-southeast oriented Grand Avenue cuts across the three cities at image center. Grand Avenue is a major transportation corridor through the western metropolitan area; the lighting patterns of large industrial and commercial properties are visible along its length. Other brightly lit properties include large shopping centers, strip malls, and gas stations, which tend to be located at the intersections of north-south and east-west trending streets.\\n\\nThe urban grid encourages growth outwards along a city's borders by providing optimal access to new real estate. Fueled by the adoption of widespread personal automobile use during the twentieth century, the Phoenix metropolitan area today includes 25 other municipalities (many of them largely suburban and residential) linked by a network of surface streets and freeways.\\n\\nWhile much of the land area highlighted in this image is urbanized, there are several noticeably dark areas. The Phoenix Mountains are largely public parks and recreational land. To the west, agricultural fields provide a sharp contrast to the lit streets of residential developments. The Salt River channel appears as a dark ribbon within the urban grid.\\n\\n\\n<!-- PageFooter=\\\"Earth at Night\\\" -->\\n<!-- PageNumber=\\\"88\\\" -->\"},{\"ref_id\":1,\"content\":\"# Urban Structure\\n\\n## March 16, 2013\\n\\n### Phoenix Metropolitan Area at Night\\n\\nThis figure presents a nighttime satellite view of the Phoenix metropolitan area, highlighting urban structure and transport corridors. City lights illuminate the layout of several cities and major thoroughfares.\\n\\n**Labeled Urban Features:**\\n\\n- **Phoenix:** Central and brightest area in the right-center of the image.\\n- **Glendale:** Located to the west of Phoenix, this city is also brightly lit.\\n- **Peoria:** Further northwest, this area is labeled and its illuminated grid is seen.\\n- **Grand Avenue:** Clearly visible as a diagonal, brightly lit thoroughfare running from Phoenix through Glendale and Peoria.\\n- **Salt River Channel:** Identified in the southeast portion, running through illuminated sections.\\n- **Phoenix Mountains:** Dark, undeveloped region to the northeast of Phoenix.\\n- **Agricultural Fields:** Southwestern corner of the image, grid patterns are visible but with much less illumination, indicating agricultural land use.\\n\\n**Additional Notes:**\\n\\n- The overall pattern shows a grid-like urban development typical of western U.S. cities, with scattered bright nodes at major intersections or city centers.\\n- There is a clear transition from dense urban development to sparsely populated or agricultural land, particularly evident towards the bottom and left of the image.\\n- The illuminated areas follow the existing road and street grids, showcasing the extensive spread of the metropolitan area.\\n\\n**Figure Description:**  \\nA satellite nighttime image captured on March 16, 2013, showing Phoenix and surrounding areas (including Glendale and Peoria). Major landscape and infrastructural features, such as the Phoenix Mountains, Grand Avenue, the Salt River Channel, and agricultural fields, are labeled. The image reveals the extent of urbanization and the characteristic street grid illuminated by city lights.\\n\\n---\\n\\nPage 89\"}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(retrievalResult.Value.Response[0].Content[0] as KnowledgeAgentMessageTextContent).Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ace2c90",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activities:\n",
      "Activity Type: KnowledgeAgentModelQueryPlanningActivityRecord\n",
      "{\n",
      "  \"InputTokens\": 1262,\n",
      "  \"OutputTokens\": 417,\n",
      "  \"ElapsedMs\": null,\n",
      "  \"Id\": 0\n",
      "}\n",
      "Activity Type: KnowledgeAgentSearchActivityRecord\n",
      "{\n",
      "  \"TargetIndex\": \"earth_at_night\",\n",
      "  \"Query\": {\n",
      "    \"Search\": \"Why do suburban belts experience more December brightening than urban cores?\",\n",
      "    \"Filter\": null\n",
      "  },\n",
      "  \"QueryTime\": \"2025-05-14T03:55:13.414+00:00\",\n",
      "  \"Count\": 0,\n",
      "  \"ElapsedMs\": 743,\n",
      "  \"Id\": 1\n",
      "}\n",
      "Activity Type: KnowledgeAgentSearchActivityRecord\n",
      "{\n",
      "  \"TargetIndex\": \"earth_at_night\",\n",
      "  \"Query\": {\n",
      "    \"Search\": \"Why is the Phoenix nighttime street grid more visible from space than interstate stretches between midwestern cities?\",\n",
      "    \"Filter\": null\n",
      "  },\n",
      "  \"QueryTime\": \"2025-05-14T03:55:13.871+00:00\",\n",
      "  \"Count\": 2,\n",
      "  \"ElapsedMs\": 445,\n",
      "  \"Id\": 2\n",
      "}\n",
      "Activity Type: KnowledgeAgentSemanticRankerActivityRecord\n",
      "{\n",
      "  \"InputTokens\": 52841,\n",
      "  \"ElapsedMs\": null,\n",
      "  \"Id\": 3\n",
      "}\n",
      "Results\n",
      "Reference Type: KnowledgeAgentAzureSearchDocReference\n",
      "{\n",
      "  \"DocKey\": \"earth_at_night_508_page_105_verbalized\",\n",
      "  \"SourceData\": {},\n",
      "  \"Id\": \"1\",\n",
      "  \"ActivitySource\": 2\n",
      "}\n",
      "Reference Type: KnowledgeAgentAzureSearchDocReference\n",
      "{\n",
      "  \"DocKey\": \"earth_at_night_508_page_104_verbalized\",\n",
      "  \"SourceData\": {},\n",
      "  \"Id\": \"0\",\n",
      "  \"ActivitySource\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "Console.WriteLine(\"Activities:\");\n",
    "foreach (var activity in retrievalResult.Value.Activity)\n",
    "{\n",
    "    Console.WriteLine($\"Activity Type: {activity.GetType().Name}\");\n",
    "    string json = JsonSerializer.Serialize(\n",
    "        activity,\n",
    "        activity.GetType(),\n",
    "        new JsonSerializerOptions { WriteIndented = true }\n",
    "    );\n",
    "    Console.WriteLine(json);\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"Results\");\n",
    "foreach (var reference in retrievalResult.Value.References)\n",
    "{\n",
    "    Console.WriteLine($\"Reference Type: {reference.GetType().Name}\");\n",
    "    string json = JsonSerializer.Serialize(\n",
    "        reference,\n",
    "        reference.GetType(),\n",
    "        new JsonSerializerOptions { WriteIndented = true }\n",
    "    );\n",
    "    Console.WriteLine(json);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec37ad",
   "metadata": {},
   "source": [
    "## Create the Azure OpenAI client\n",
    "\n",
    "So far, this notebook has used agentic retrieval for answer *extraction*, which you can extend to answer *generation* by using the Azure OpenAI client. This enables more detailed, context-rich responses that aren't strictly tied to indexed content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "482d538d",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using Azure.AI.OpenAI;\n",
    "using OpenAI.Chat;\n",
    "\n",
    "AzureOpenAIClient azureClient = new(\n",
    "    new Uri(azureOpenAiEndpoint),\n",
    "    new DefaultAzureCredential());\n",
    "ChatClient chatClient = azureClient.GetChatClient(azureOpenAiGptDeployment);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785ec6c",
   "metadata": {},
   "source": [
    "### Use the Chat Completions API to generate an answer\n",
    "\n",
    "One option for answer generation is the Chat Completions API, which passes the conversation history to the LLM for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a112a69",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ASSISTANT]: Suburban belts display larger December brightening than urban cores because suburban areas tend to have more holiday lighting and decorations, which significantly increase visible brightness during the holiday season\n",
      " In urban cores, where light levels are already high due to existing infrastructure and city lights, the addition of holiday lighting might not have as noticeable an impact relative to existing light intensity [ref_id:0]\n",
      "\n",
      "\n",
      "The Phoenix nighttime street grid is sharply visible from space because its metropolitan area is laid out in a regular grid pattern, which is emphasized by the street lighting\n",
      " Major intersections, commercial properties, and other brightly lit areas contribute to this visibility\n",
      " In contrast, large stretches of interstate between midwestern cities do not have such concentrated sources of lighting and are often less populated, resulting in comparatively dim appearances from space [ref_id:1]\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "List<ChatMessage> chatMessages = messages\n",
    "    .Select<Dictionary<string, string>, ChatMessage>(m => m[\"role\"] switch\n",
    "    {\n",
    "        \"user\" => new UserChatMessage(m[\"content\"]),\n",
    "        \"assistant\" => new AssistantChatMessage(m[\"content\"]),\n",
    "        \"system\" => new SystemChatMessage(m[\"content\"]),\n",
    "        _ => null\n",
    "    })\n",
    "    .Where(m => m != null)\n",
    "    .ToList();\n",
    "\n",
    "\n",
    "var result = await chatClient.CompleteChatAsync(chatMessages);\n",
    "\n",
    "Console.WriteLine($\"[ASSISTANT]: {result.Value.Content[0].Text.Replace(\".\", \"\\n\")}\");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27116bc",
   "metadata": {},
   "source": [
    "## Continue the conversation\n",
    "\n",
    "This step continues the conversation with the knowledge agent, building upon the previous messages and queries to retrieve relevant information from your search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e241e04a",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "messages.Add(new Dictionary<string, string>\n",
    "{\n",
    "    { \"role\", \"user\" },\n",
    "    { \"content\", \"How do I find lava at night?\" }\n",
    "});\n",
    "\n",
    "\n",
    "var retrievalResult = await agentClient.RetrieveAsync(\n",
    "    retrievalRequest: new KnowledgeAgentRetrievalRequest(\n",
    "            messages: messages\n",
    "                .Where(message => message[\"role\"] != \"system\")\n",
    "                .Select(\n",
    "                message => new KnowledgeAgentMessage(\n",
    "                    role: message[\"role\"],\n",
    "                    content: new[] { new KnowledgeAgentMessageTextContent(message[\"content\"]) }))\n",
    "                .ToList()\n",
    "            )\n",
    "        {\n",
    "            TargetIndexParams = { new KnowledgeAgentIndexParams { IndexName = indexName, RerankerThreshold = 2.5f } }\n",
    "        }\n",
    "    );\n",
    "\n",
    "messages.Add(new Dictionary<string, string>\n",
    "{\n",
    "    { \"role\", \"assistant\" },\n",
    "    { \"content\", (retrievalResult.Value.Response[0].Content[0] as KnowledgeAgentMessageTextContent).Text }\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96373661",
   "metadata": {},
   "source": [
    "### Review the retrieval response, activity, and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9927d10",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\"ref_id\":0,\"content\":\"<!-- PageHeader=\\\"Volcanoes\\\" -->\\n\\n### Nighttime Glow at Mount Etna - Italy\\n\\nAt about 2:30 a.m. local time on March 16, 2017, the VIIRS DNB on the Suomi NPP satellite captured this nighttime image of lava flowing on Mount Etna in Sicily, Italy. Etna is one of the world's most active volcanoes.\\n\\n#### Figure: Location of Mount Etna\\nA world globe is depicted, with a marker indicating the location of Mount Etna in Sicily, Italy, in southern Europe near the center of the Mediterranean Sea.\\n\\n<!-- PageFooter=\\\"Earth at Night\\\" -->\\n<!-- PageNumber=\\\"48\\\" -->\"},{\"ref_id\":1,\"content\":\"## Nature's Light Shows\\n\\nAt night, with the light of the Sun removed, nature's brilliant glow from Earth's surface becomes visible to the naked eye from space. Some of Earth's most spectacular light shows are natural, like the aurora borealis, or Northern Lights, in the Northern Hemisphere (aurora australis, or Southern Lights, in the Southern Hemisphere). The auroras are natural electrical phenomena caused by charged particles that race from the Sun toward Earth, inducing chemical reactions in the upper atmosphere and creating the appearance of streamers of reddish or greenish light in the sky, usually near the northern or southern magnetic pole. Other natural lights can indicate danger, like a raging forest fire encroaching on a city, town, or community, or lava spewing from an erupting volcano.\\n\\nWhatever the source, the ability of humans to monitor nature's light shows at night has practical applications for society. For example, tracking fires during nighttime hours allows for continuous monitoring and enhances our ability to protect humans and other animals, plants, and infrastructure. Combined with other data sources, our ability to observe the light of fires at night allows emergency managers to more efficiently and accurately issue warnings and evacuation orders and allows firefighting efforts to continue through the night. With enough moonlight (e.g., full-Moon phase), it's even possible to track the movement of smoke plumes at night, which can impact air quality, regardless of time of day.\\n\\nAnother natural source of light at night is emitted from glowing lava flows at the site of active volcanoes. Again, with enough moonlight, these dramatic scenes can be tracked and monitored for both scientific research and public safety.\\n\\n\\n### Figure: The Northern Lights Viewed from Space\\n\\n**September 17, 2011**\\n\\nThis photo, taken from the International Space Station on September 17, 2011, shows a spectacular display of the aurora borealis (Northern Lights) as green and reddish light in the night sky above Earth. In the foreground, part of a Soyuz spacecraft is visible, silhouetted against the bright auroral light. The green glow is generated by energetic charged particles from the Sun interacting with Earth's upper atmosphere, exciting oxygen and nitrogen atoms, and producing characteristic colors. The image demonstrates the vividness and grandeur of natural night-time light phenomena as seen from orbit.\"},{\"ref_id\":2,\"content\":\"For the first time in perhaps a decade, Mount Etna experienced a \\\"flank eruption\\\"—erupting from its side instead of its summit—on December 24, 2018. The activity was accompanied by 130 earthquakes occurring over three hours that morning. Mount Etna, Europe’s most active volcano, has seen periodic activity on this part of the mountain since 2013. The Operational Land Imager (OLI) on the Landsat 8 satellite acquired the main image of Mount Etna on December 28, 2018.\\n\\nThe inset image highlights the active vent and thermal infrared signature from lava flows, which can be seen near the newly formed fissure on the southeastern side of the volcano. The inset was created with data from OLI and the Thermal Infrared Sensor (TIRS) on Landsat 8. Ash spewing from the fissure cloaked adjacent villages and delayed aircraft from landing at the nearby Catania airport. Earthquakes occurred in the subsequent days after the initial eruption and displaced hundreds of people from their homes.\\n\\nFor nighttime images of Mount Etna’s March 2017 eruption, see pages 48–51.\\n\\n---\\n\\n### Hazards of Volcanic Ash Plumes and Satellite Observation\\n\\nWith the help of moonlight, satellite instruments can track volcanic ash plumes, which present significant hazards to airplanes in flight. The volcanic ash—composed of tiny pieces of glass and rock—is abrasive to engine turbine blades, and can melt on the blades and other engine parts, causing damage and even engine stalls. This poses a danger to both the plane’s integrity and passenger safety. Volcanic ash also reduces visibility for pilots and can cause etching of windshields, further reducing pilots’ ability to see. Nightlight images can be combined with thermal images to provide a more complete view of volcanic activity on Earth’s surface.\\n\\nThe VIIRS Day/Night Band (DNB) on polar-orbiting satellites uses faint light sources such as moonlight, airglow (the atmosphere’s self-illumination through chemical reactions), zodiacal light (sunlight scattered by interplanetary dust), and starlight from the Milky Way. Using these dim light sources, the DNB can detect changes in clouds, snow cover, and sea ice:\\n\\n#### Table: Light Sources Used by VIIRS DNB\\n\\n| Light Source         | Description                                                                  |\\n|----------------------|------------------------------------------------------------------------------|\\n| Moonlight            | Reflected sunlight from the Moon, illuminating Earth's surface at night      |\\n| Airglow              | Atmospheric self-illumination from chemical reactions                        |\\n| Zodiacal Light       | Sunlight scattered by interplanetary dust                                    |\\n| Starlight/Milky Way  | Faint illumination provided by stars in the Milky Way                        |\\n\\nGeostationary Operational Environmental Satellites (GOES), managed by NOAA, orbit over Earth’s equator and offer uninterrupted observations of North America. High-latitude areas such as Alaska benefit from polar-orbiting satellites like Suomi NPP, which provide overlapping coverage at the poles, enabling more data collection in these regions. During polar darkness (winter months), VIIRS DNB data allow scientists to:\\n\\n- Observe sea ice formation\\n- Monitor snow cover extent at the highest latitudes\\n- Detect open water for ship navigation\\n\\n#### Table: Satellite Coverage Overview\\n\\n| Satellite Type          | Orbit           | Coverage Area         | Special Utility                              |\\n|------------------------|-----------------|----------------------|----------------------------------------------|\\n| GOES                   | Geostationary   | Equatorial/North America | Continuous regional monitoring              |\\n| Polar-Orbiting (e.g., Suomi NPP) | Polar-orbiting    | Poles/high latitudes      | Overlapping passes; useful during polar night|\\n\\n---\\n\\n### Weather Forecasting and Nightlight Data\\n\\nThe use of nightlight data by weather forecasters is growing as the VIIRS instrument enables observation of clouds at night illuminated by sources such as moonlight and lightning. Scientists use these data to study the nighttime behavior of weather systems, including severe storms, which can develop and strike populous areas at night as well as during the day. Combined with thermal data, visible nightlight data allow the detection of clouds at various heights in the atmosphere, such as dense marine fog. This capability enables weather forecasters to issue marine advisories with higher confidence, leading to greater utility. (See \\\"Marine Layer Clouds—California\\\" on page 56.)\\n\\nIn this section of the book, you will see how nightlight data are used to observe nature’s spectacular light shows across a wide range of sources.\\n\\n---\\n\\n#### Notable Data from Mount Etna Flank Eruption (December 2018)\\n\\n| Event/Observation                  | Details                                                                    |\\n|-------------------------------------|----------------------------------------------------------------------------|\\n| Date of Flank Eruption              | December 24, 2018                                                          |\\n| Number of Earthquakes               | 130 earthquakes within 3 hours                                              |\\n| Image Acquisition                   | December 28, 2018 by Landsat 8 OLI                                         |\\n| Location of Eruption                | Southeastern side of Mount Etna                                            |\\n| Thermal Imaging Data                | From OLI and TIRS (Landsat 8), highlighting active vent and lava flows     |\\n| Impact on Villages/Air Transport    | Ash covered villages; delayed aircraft at Catania airport                  |\\n| Displacement                        | Hundreds of residents displaced                                            |\\n| Ongoing Seismic Activity            | Earthquakes continued after initial eruption                               |\\n\\n---\\n\\n<!-- PageFooter=\\\"Earth at Night\\\" -->\\n<!-- PageNumber=\\\"30\\\" -->\"},{\"ref_id\":3,\"content\":\"<!-- PageHeader=\\\"Volcanoes\\\" -->\\n\\n## Volcanoes\\n\\n### The Infrared Glows of Kilauea's Lava Flows—Hawaii\\n\\nIn early May 2018, an eruption on Hawaii's Kilauea volcano began to unfold. The eruption took a dangerous turn on May 3, 2018, when new fissures opened in the residential neighborhood of Leilani Estates. During the summer-long eruptive event, other fissures emerged along the East Rift Zone. Lava from vents along the rift zone flowed downslope, reaching the ocean in several areas, and filling in Kapoho Bay.\\n\\nA time series of Landsat 8 imagery shows the progression of the lava flows from May 16 to August 13. The night view combines thermal, shortwave infrared, and near-infrared wavelengths to tease out the very hot lava (bright white), cooling lava (red), and lava flows obstructed by clouds (purple).\\n\\n#### Figure: Location of Kilauea Volcano, Hawaii\\n\\nA globe is shown centered on North America, with a marker placed in the Pacific Ocean indicating the location of Hawaii, to the southwest of the mainland United States.\\n\\n<!-- PageFooter=\\\"Earth at Night\\\" -->\\n<!-- PageNumber=\\\"44\\\" -->\"}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(retrievalResult.Value.Response[0].Content[0] as KnowledgeAgentMessageTextContent).Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "462137b4",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activities:\n",
      "Activity Type: KnowledgeAgentModelQueryPlanningActivityRecord\n",
      "{\n",
      "  \"InputTokens\": 2138,\n",
      "  \"OutputTokens\": 316,\n",
      "  \"ElapsedMs\": null,\n",
      "  \"Id\": 0\n",
      "}\n",
      "Activity Type: KnowledgeAgentSearchActivityRecord\n",
      "{\n",
      "  \"TargetIndex\": \"earth_at_night\",\n",
      "  \"Query\": {\n",
      "    \"Search\": \"methods to find lava flows at night\",\n",
      "    \"Filter\": null\n",
      "  },\n",
      "  \"QueryTime\": \"2025-05-14T03:55:25.211+00:00\",\n",
      "  \"Count\": 4,\n",
      "  \"ElapsedMs\": 393,\n",
      "  \"Id\": 1\n",
      "}\n",
      "Activity Type: KnowledgeAgentSearchActivityRecord\n",
      "{\n",
      "  \"TargetIndex\": \"earth_at_night\",\n",
      "  \"Query\": {\n",
      "    \"Search\": \"equipment for detecting lava at night\",\n",
      "    \"Filter\": null\n",
      "  },\n",
      "  \"QueryTime\": \"2025-05-14T03:55:25.548+00:00\",\n",
      "  \"Count\": 0,\n",
      "  \"ElapsedMs\": 324,\n",
      "  \"Id\": 2\n",
      "}\n",
      "Activity Type: KnowledgeAgentSearchActivityRecord\n",
      "{\n",
      "  \"TargetIndex\": \"earth_at_night\",\n",
      "  \"Query\": {\n",
      "    \"Search\": \"safety tips for observing lava at night\",\n",
      "    \"Filter\": null\n",
      "  },\n",
      "  \"QueryTime\": \"2025-05-14T03:55:25.865+00:00\",\n",
      "  \"Count\": 0,\n",
      "  \"ElapsedMs\": 317,\n",
      "  \"Id\": 3\n",
      "}\n",
      "Activity Type: KnowledgeAgentSemanticRankerActivityRecord\n",
      "{\n",
      "  \"InputTokens\": 72789,\n",
      "  \"ElapsedMs\": null,\n",
      "  \"Id\": 4\n",
      "}\n",
      "Results\n",
      "Reference Type: KnowledgeAgentAzureSearchDocReference\n",
      "{\n",
      "  \"DocKey\": \"earth_at_night_508_page_60_verbalized\",\n",
      "  \"SourceData\": {},\n",
      "  \"Id\": \"3\",\n",
      "  \"ActivitySource\": 1\n",
      "}\n",
      "Reference Type: KnowledgeAgentAzureSearchDocReference\n",
      "{\n",
      "  \"DocKey\": \"earth_at_night_508_page_46_verbalized\",\n",
      "  \"SourceData\": {},\n",
      "  \"Id\": \"2\",\n",
      "  \"ActivitySource\": 1\n",
      "}\n",
      "Reference Type: KnowledgeAgentAzureSearchDocReference\n",
      "{\n",
      "  \"DocKey\": \"earth_at_night_508_page_44_verbalized\",\n",
      "  \"SourceData\": {},\n",
      "  \"Id\": \"1\",\n",
      "  \"ActivitySource\": 1\n",
      "}\n",
      "Reference Type: KnowledgeAgentAzureSearchDocReference\n",
      "{\n",
      "  \"DocKey\": \"earth_at_night_508_page_64_verbalized\",\n",
      "  \"SourceData\": {},\n",
      "  \"Id\": \"0\",\n",
      "  \"ActivitySource\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "Console.WriteLine(\"Activities:\");\n",
    "foreach (var activity in retrievalResult.Value.Activity)\n",
    "{\n",
    "    Console.WriteLine($\"Activity Type: {activity.GetType().Name}\");\n",
    "    string json = JsonSerializer.Serialize(\n",
    "        activity,\n",
    "        activity.GetType(),\n",
    "        new JsonSerializerOptions { WriteIndented = true }\n",
    "    );\n",
    "    Console.WriteLine(json);\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"Results\");\n",
    "foreach (var reference in retrievalResult.Value.References)\n",
    "{\n",
    "    Console.WriteLine($\"Reference Type: {reference.GetType().Name}\");\n",
    "    string json = JsonSerializer.Serialize(\n",
    "        reference,\n",
    "        reference.GetType(),\n",
    "        new JsonSerializerOptions { WriteIndented = true }\n",
    "    );\n",
    "    Console.WriteLine(json);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb2da5b",
   "metadata": {},
   "source": [
    "## Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cf2af8e",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ASSISTANT]: To find lava at night, one effective method is using satellite technology that detects infrared or infrared heat signatures\n",
      " For instance, the VIIRS Day/Night Band (DNB) on satellites can detect glowing lava flows by capturing natural nightlight sources, such as moonlight, which highlight volcanic activity\n",
      " Additionally, thermal imaging satellites like Landsat can capture detailed images of lava flows using thermal, shortwave infrared, and near-infrared wavelengths, providing visibility of hot and cooling lava flows even through obstructions like clouds\n",
      " This data can then be analyzed to pinpoint active lava locations at night, making it both possible and practical for scientists and emergency responders to monitor volcanic activity remotely ([ref_id: 0, 1, 2, 3])\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "List<ChatMessage> chatMessages = messages\n",
    "    .Select<Dictionary<string, string>, ChatMessage>(m => m[\"role\"] switch\n",
    "    {\n",
    "        \"user\" => new UserChatMessage(m[\"content\"]),\n",
    "        \"assistant\" => new AssistantChatMessage(m[\"content\"]),\n",
    "        \"system\" => new SystemChatMessage(m[\"content\"]),\n",
    "        _ => null\n",
    "    })\n",
    "    .Where(m => m != null)\n",
    "    .ToList();\n",
    "\n",
    "\n",
    "var result = await chatClient.CompleteChatAsync(chatMessages);\n",
    "\n",
    "Console.WriteLine($\"[ASSISTANT]: {result.Value.Content[0].Text.Replace(\".\", \"\\n\")}\");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80883384",
   "metadata": {},
   "source": [
    "## Clean up objects and resources\n",
    "\n",
    "If you no longer need Azure AI Search or Azure OpenAI, delete them from your Azure subscription. You can also start over by deleting individual objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3532f3",
   "metadata": {},
   "source": [
    "### Delete the knowledge agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bb888",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search agent 'earth-search-agent' deleted successfully\r\n"
     ]
    }
   ],
   "source": [
    "await indexClient.DeleteKnowledgeAgentAsync(agentName);\n",
    "System.Console.WriteLine($\"Knowledge agent '{agentName}' deleted successfully\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e88e9c",
   "metadata": {},
   "source": [
    "### Delete the search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5c709f0",
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'earth_at_night' deleted successfully\r\n"
     ]
    }
   ],
   "source": [
    "await indexClient.DeleteIndexAsync(indexName);\n",
    "System.Console.WriteLine($\"Index '{indexName}' deleted successfully\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "python"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
